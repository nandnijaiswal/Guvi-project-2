# -*- coding: utf-8 -*-
"""school_result_analyzer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yMjLByXjRP3sksibfC4KCQY8eqzc92z5

# School Result Data Analyzer .
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

"""# 1. Load Dataset"""

df = pd.read_csv("/content/drive/MyDrive/school_results_cleaned.csv")

"""# 2. Data Cleaning


"""

df.drop_duplicates(inplace=True)

subjects = ["Hindi", "English", "Science", "Maths", "History", "Geography"]
df[subjects] = df[subjects].astype(int)

df["Total"] = df[subjects].sum(axis=1)
df["Pass/Fail"] = df["Total"].apply(lambda x: "Pass" if (x/len(subjects)) >= 40 else "Fail")

"""

 # 3. Subject-wise Analysis


"""

subject_avg = df[subjects].mean().round(2)
subject_topper = {sub: df.loc[df[sub].idxmax(), "Student"] for sub in subjects}

print("\n--- Subject Averages ---")
print(subject_avg)

print("\n--- Subject Toppers ---")
for sub, stu in subject_topper.items():
    marks = df.loc[df[sub].idxmax(), sub]
    print(f"{sub} â†’ {stu} ({marks} marks)")

"""# 4. Class-wise Toppers"""

class_toppers = df.groupby("Class").apply(lambda x: x.loc[x["Total"].idxmax()][["Student", "Total"]])

print("\n--- Class-wise Toppers ---")
print(class_toppers)

# 5. Pass/Fail Rate
pass_rate = round((df['Pass/Fail'].value_counts(normalize=True)['Pass']*100),2)
print(f"\nPass Rate: {pass_rate}%")

"""# 6. Visualizations"""

plt.figure(figsize=(8,5))
subject_avg.plot(kind="bar", color="skyblue", edgecolor="black")
plt.title("Subject-wise Average Marks")
plt.ylabel("Average Marks")
plt.xticks(rotation=0)
plt.tight_layout()
plt.show()

plt.figure(figsize=(5,5))
df["Pass/Fail"].value_counts().plot(kind="pie", autopct='%1.1f%%', startangle=90, colors=["lightgreen", "salmon"])
plt.title("Pass vs Fail Distribution")
plt.ylabel("")
plt.show()

plt.figure(figsize=(7,5))
sns.heatmap(df[subjects].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Between Subjects")
plt.show()

"""# 7. ML Model: Predict Pass/Fail"""

X = df[subjects]
y = df["Pass/Fail"].map({"Pass":1, "Fail":0})

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# Evaluation Metrics
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)

print("\n--- Model Evaluation ---")
print(f"Accuracy: {acc:.2f}")
print(f"Precision: {prec:.2f}")
print(f"Recall: {rec:.2f}")
print(f"F1 Score: {f1:.2f}")
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""#8. Export Summary Report"""

report = {
    "Subject Averages": subject_avg.to_dict(),
    "Subject Toppers": subject_topper,
    "Class Toppers": class_toppers.to_dict(),
    "Pass Rate (%)": pass_rate,
    "Model Accuracy": round(acc,2),
    "Model Precision": round(prec,2),
    "Model Recall": round(rec,2),
    "Model F1": round(f1,2)
}

with open("final_report.txt", "w") as f:
    for key, value in report.items():
        f.write(f"{key}: {value}\n")

print("\n Project complete. Summary saved to 'final_report.txt'")